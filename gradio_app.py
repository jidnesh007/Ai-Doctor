import os
import gradio as gr
from brain import encode_image, analyze_image_with_query
from voice import record_audio, transcribe_with_groq
from doc_voice import text_to_speech_with_gtts, text_to_speech_with_elevenlabs

# Enhanced TTS function with language support
def text_to_speech_multilingual(input_text, output_filepath, language='en', use_elevenlabs=False):
    """
    Enhanced text-to-speech with Hindi and Marathi support
    
    Args:
        input_text: Text to convert to speech
        output_filepath: Path to save audio file
        language: Language code ('en', 'hi', 'mr')
        use_elevenlabs: Whether to use ElevenLabs (True) or gTTS (False)
    """
    if use_elevenlabs:
        # ElevenLabs supports multilingual voices but requires specific voice IDs
        # You'll need to find Hindi/Marathi voice IDs from ElevenLabs
        return text_to_speech_with_elevenlabs(input_text, output_filepath)
    else:
        # Use gTTS with language support
        from gtts import gTTS
        import subprocess
        import platform
        
        # Language mapping
        lang_codes = {
            'english': 'en',
            'hindi': 'hi', 
            'marathi': 'mr'
        }
        
        lang_code = lang_codes.get(language, 'en')
        
        try:
            audioobj = gTTS(
                text=input_text,
                lang=lang_code,
                slow=False
            )
            audioobj.save(output_filepath)
            
            # Auto-play the audio (same as your existing code)
            os_name = platform.system()
            if os_name == "Darwin":  # macOS
                subprocess.run(['afplay', output_filepath])
            elif os_name == "Windows":  # Windows
                subprocess.run(['powershell', '-c', f'Add-Type -AssemblyName presentationCore; $mediaPlayer = New-Object system.windows.media.mediaplayer; $mediaPlayer.open([uri]::new((Resolve-Path "{output_filepath}").Path)); $mediaPlayer.Play(); Start-Sleep 5'])
            elif os_name == "Linux":  # Linux
                players = ['mpg123', 'ffplay', 'aplay']
                for player in players:
                    try:
                        subprocess.run([player, output_filepath], check=True)
                        break
                    except (subprocess.CalledProcessError, FileNotFoundError):
                        continue
                else:
                    raise OSError("No suitable audio player found")
                    
        except Exception as e:
            print(f"Error in multilingual TTS: {e}")
            return None

# Enhanced system prompts for different languages
system_prompts = {
    'english': """You have to act as a professional doctor, i know you are not but this is for learning purpose.
             What's in this image?. Do you find anything wrong with it medically?
             If you make a differential, suggest some remedies for them. Donot add any numbers or special characters in
             your response. Your response should be in one long paragraph. Also always answer as if you are answering to a real person.
            Donot say 'In the image I see' but say 'With what I see, I think you have ....'
            Dont respond as an AI model in markdown, your answer should mimic that of an actual doctor not an AI bot,
             Keep your answer concise (max 2 sentences). No preamble, start your answer right away please""",
    
    'hindi': """‡§Ü‡§™‡§ï‡•ã ‡§è‡§ï ‡§™‡•á‡§∂‡•á‡§µ‡§∞ ‡§°‡•â‡§ï‡•ç‡§ü‡§∞ ‡§ï‡•Ä ‡§§‡§∞‡§π ‡§ï‡§æ‡§Æ ‡§ï‡§∞‡§®‡§æ ‡§π‡•à, ‡§Æ‡•Å‡§ù‡•á ‡§™‡§§‡§æ ‡§π‡•à ‡§ï‡§ø ‡§Ü‡§™ ‡§®‡§π‡•Ä‡§Ç ‡§π‡•à‡§Ç ‡§≤‡•á‡§ï‡§ø‡§® ‡§Ø‡§π ‡§∏‡•Ä‡§ñ‡§®‡•á ‡§ï‡•á ‡§â‡§¶‡•ç‡§¶‡•á‡§∂‡•ç‡§Ø ‡§∏‡•á ‡§π‡•à‡•§
             ‡§á‡§∏ ‡§§‡§∏‡•ç‡§µ‡•Ä‡§∞ ‡§Æ‡•á‡§Ç ‡§ï‡•ç‡§Ø‡§æ ‡§π‡•à? ‡§ï‡•ç‡§Ø‡§æ ‡§Ü‡§™‡§ï‡•ã ‡§á‡§∏‡§Æ‡•á‡§Ç ‡§ö‡§ø‡§ï‡§ø‡§§‡•ç‡§∏‡§æ ‡§ï‡•Ä ‡§¶‡•É‡§∑‡•ç‡§ü‡§ø ‡§∏‡•á ‡§ï‡•Å‡§õ ‡§ó‡§≤‡§§ ‡§≤‡§ó‡§§‡§æ ‡§π‡•à?
             ‡§Ø‡§¶‡§ø ‡§Ü‡§™ ‡§ï‡•ã‡§à ‡§®‡§ø‡§¶‡§æ‡§® ‡§ï‡§∞‡§§‡•á ‡§π‡•à‡§Ç, ‡§§‡•ã ‡§â‡§∏‡§ï‡•á ‡§≤‡§ø‡§è ‡§ï‡•Å‡§õ ‡§â‡§™‡§ö‡§æ‡§∞ ‡§∏‡•Å‡§ù‡§æ‡§è‡§Ç‡•§ ‡§Ö‡§™‡§®‡•á ‡§â‡§§‡•ç‡§§‡§∞ ‡§Æ‡•á‡§Ç ‡§ï‡•ã‡§à ‡§∏‡§Ç‡§ñ‡•ç‡§Ø‡§æ ‡§Ø‡§æ ‡§µ‡§ø‡§∂‡•á‡§∑ ‡§µ‡§∞‡•ç‡§£ ‡§® ‡§ú‡•ã‡§°‡§º‡•á‡§Ç‡•§
             ‡§Ü‡§™‡§ï‡§æ ‡§â‡§§‡•ç‡§§‡§∞ ‡§è‡§ï ‡§≤‡§Ç‡§¨‡•á ‡§™‡•à‡§∞‡§æ‡§ó‡•ç‡§∞‡§æ‡§´ ‡§Æ‡•á‡§Ç ‡§π‡•ã‡§®‡§æ ‡§ö‡§æ‡§π‡§ø‡§è‡•§ ‡§π‡§Æ‡•á‡§∂‡§æ ‡§ê‡§∏‡•á ‡§â‡§§‡•ç‡§§‡§∞ ‡§¶‡•á‡§Ç ‡§ú‡•à‡§∏‡•á ‡§Ü‡§™ ‡§ï‡§ø‡§∏‡•Ä ‡§µ‡§æ‡§∏‡•ç‡§§‡§µ‡§ø‡§ï ‡§µ‡•ç‡§Ø‡§ï‡•ç‡§§‡§ø ‡§ï‡•ã ‡§ú‡§µ‡§æ‡§¨ ‡§¶‡•á ‡§∞‡§π‡•á ‡§π‡•ã‡§Ç‡•§
             '‡§§‡§∏‡•ç‡§µ‡•Ä‡§∞ ‡§Æ‡•á‡§Ç ‡§Æ‡•Å‡§ù‡•á ‡§¶‡§ø‡§ñ‡§§‡§æ ‡§π‡•à' ‡§® ‡§ï‡§π‡•á‡§Ç ‡§¨‡§≤‡•ç‡§ï‡§ø '‡§ú‡•ã ‡§Æ‡•Å‡§ù‡•á ‡§¶‡§ø‡§ñ‡§§‡§æ ‡§π‡•à, ‡§Æ‡•Å‡§ù‡•á ‡§≤‡§ó‡§§‡§æ ‡§π‡•à ‡§ï‡§ø ‡§Ü‡§™‡§ï‡•ã... ‡§π‡•à'
             AI ‡§Æ‡•â‡§°‡§≤ ‡§ï‡•Ä ‡§§‡§∞‡§π ‡§Æ‡§æ‡§∞‡•ç‡§ï‡§°‡§æ‡§â‡§® ‡§Æ‡•á‡§Ç ‡§ú‡§µ‡§æ‡§¨ ‡§® ‡§¶‡•á‡§Ç, ‡§Ü‡§™‡§ï‡§æ ‡§â‡§§‡•ç‡§§‡§∞ ‡§µ‡§æ‡§∏‡•ç‡§§‡§µ‡§ø‡§ï ‡§°‡•â‡§ï‡•ç‡§ü‡§∞ ‡§ï‡§æ ‡§π‡•ã‡§®‡§æ ‡§ö‡§æ‡§π‡§ø‡§è ‡§® ‡§ï‡§ø AI ‡§¨‡•â‡§ü ‡§ï‡§æ‡•§
             ‡§Ö‡§™‡§®‡§æ ‡§â‡§§‡•ç‡§§‡§∞ ‡§∏‡§Ç‡§ï‡•ç‡§∑‡§ø‡§™‡•ç‡§§ ‡§∞‡§ñ‡•á‡§Ç (‡§Ö‡§ß‡§ø‡§ï‡§§‡§Æ 2 ‡§µ‡§æ‡§ï‡•ç‡§Ø)‡•§ ‡§ï‡•ã‡§à ‡§™‡•ç‡§∞‡§∏‡•ç‡§§‡§æ‡§µ‡§®‡§æ ‡§®‡§π‡•Ä‡§Ç, ‡§Ö‡§™‡§®‡§æ ‡§â‡§§‡•ç‡§§‡§∞ ‡§§‡•Å‡§∞‡§Ç‡§§ ‡§∂‡•Å‡§∞‡•Ç ‡§ï‡§∞‡•á‡§Ç‡•§ ‡§π‡§ø‡§Ç‡§¶‡•Ä ‡§Æ‡•á‡§Ç ‡§â‡§§‡•ç‡§§‡§∞ ‡§¶‡•á‡§Ç‡•§""",
    
    'marathi': """‡§§‡•Å‡§Æ‡•ç‡§π‡§æ‡§≤‡§æ ‡§è‡§ï ‡§µ‡•ç‡§Ø‡§æ‡§µ‡§∏‡§æ‡§Ø‡§ø‡§ï ‡§°‡•â‡§ï‡•ç‡§ü‡§∞ ‡§Æ‡•ç‡§π‡§£‡•Ç‡§® ‡§ï‡§æ‡§Æ ‡§ï‡§∞‡§æ‡§µ‡•á ‡§≤‡§æ‡§ó‡•á‡§≤, ‡§Æ‡§≤‡§æ ‡§Æ‡§æ‡§π‡§ø‡§§ ‡§Ü‡§π‡•á ‡§ï‡•Ä ‡§§‡•Å‡§Æ‡•ç‡§π‡•Ä ‡§®‡§æ‡§π‡•Ä ‡§™‡§£ ‡§π‡•á ‡§∂‡§ø‡§ï‡§£‡•ç‡§Ø‡§æ‡§ö‡•ç‡§Ø‡§æ ‡§â‡§¶‡•ç‡§¶‡•á‡§∂‡§æ‡§®‡•á ‡§Ü‡§π‡•á‡•§
             ‡§Ø‡§æ ‡§ö‡§ø‡§§‡•ç‡§∞‡§æ‡§§ ‡§ï‡§æ‡§Ø ‡§Ü‡§π‡•á? ‡§§‡•Å‡§Æ‡•ç‡§π‡§æ‡§≤‡§æ ‡§Ø‡§æ‡§§ ‡§µ‡•à‡§¶‡•ç‡§Ø‡§ï‡•Ä‡§Ø ‡§¶‡•É‡§∑‡•ç‡§ü‡•Ä‡§®‡•á ‡§ï‡§æ‡§π‡•Ä ‡§ö‡•Å‡§ï‡•Ä‡§ö‡§Ç ‡§µ‡§æ‡§ü‡§§‡§Ç ‡§ï‡§æ?
             ‡§ú‡§∞ ‡§§‡•Å‡§Æ‡•ç‡§π‡•Ä ‡§®‡§ø‡§¶‡§æ‡§® ‡§ï‡§∞‡§§‡§æ ‡§§‡§∞ ‡§§‡•ç‡§Ø‡§æ‡§∏‡§æ‡§†‡•Ä ‡§ï‡§æ‡§π‡•Ä ‡§â‡§™‡§ö‡§æ‡§∞ ‡§∏‡•Å‡§ö‡§µ‡§æ. ‡§§‡•Å‡§Æ‡§ö‡•ç‡§Ø‡§æ ‡§â‡§§‡•ç‡§§‡§∞‡§æ‡§§ ‡§ï‡•ã‡§£‡§§‡•á‡§π‡•Ä ‡§∏‡§Ç‡§ñ‡•ç‡§Ø‡§æ ‡§ï‡§ø‡§Ç‡§µ‡§æ ‡§µ‡§ø‡§∂‡•á‡§∑ ‡§µ‡§∞‡•ç‡§£ ‡§ú‡•ã‡§°‡•Ç ‡§®‡§ï‡§æ.
             ‡§§‡•Å‡§Æ‡§ö‡•á ‡§â‡§§‡•ç‡§§‡§∞ ‡§è‡§ï‡§æ ‡§≤‡§æ‡§Ç‡§¨ ‡§™‡§∞‡§ø‡§ö‡•ç‡§õ‡•á‡§¶‡§æ‡§§ ‡§Ö‡§∏‡§æ‡§µ‡•á. ‡§®‡•á‡§π‡§Æ‡•Ä ‡§Ö‡§∏‡•á ‡§â‡§§‡•ç‡§§‡§∞ ‡§¶‡•ç‡§Ø‡§æ ‡§ú‡§∏‡•á ‡§§‡•Å‡§Æ‡•ç‡§π‡•Ä ‡§ñ‡§±‡•ç‡§Ø‡§æ ‡§µ‡•ç‡§Ø‡§ï‡•ç‡§§‡•Ä‡§≤‡§æ ‡§â‡§§‡•ç‡§§‡§∞ ‡§¶‡•á‡§§ ‡§Ü‡§π‡§æ‡§§.
             '‡§ö‡§ø‡§§‡•ç‡§∞‡§æ‡§§ ‡§Æ‡§≤‡§æ ‡§¶‡§ø‡§∏‡§§‡§Ç' ‡§Ö‡§∏‡§Ç ‡§Æ‡•ç‡§π‡§£‡•Ç ‡§®‡§ï‡§æ ‡§§‡§∞ '‡§ú‡•á ‡§Æ‡§≤‡§æ ‡§¶‡§ø‡§∏‡§§‡§Ç ‡§§‡•ç‡§Ø‡§æ‡§µ‡§∞‡•Ç‡§® ‡§Æ‡§≤‡§æ ‡§µ‡§æ‡§ü‡§§‡§Ç ‡§§‡•Å‡§Æ‡•ç‡§π‡§æ‡§≤‡§æ... ‡§Ü‡§π‡•á'
             AI ‡§Æ‡•â‡§°‡•á‡§≤ ‡§∏‡§æ‡§∞‡§ñ‡§Ç ‡§Æ‡§æ‡§∞‡•ç‡§ï‡§°‡§æ‡§â‡§®‡§Æ‡§ß‡•ç‡§Ø‡•á ‡§â‡§§‡•ç‡§§‡§∞ ‡§¶‡•á‡§ä ‡§®‡§ï‡§æ, ‡§§‡•Å‡§Æ‡§ö‡§Ç ‡§â‡§§‡•ç‡§§‡§∞ ‡§ñ‡§±‡•ç‡§Ø‡§æ ‡§°‡•â‡§ï‡•ç‡§ü‡§∞‡§æ‡§∏‡§æ‡§∞‡§ñ‡§Ç ‡§Ö‡§∏‡§æ‡§µ‡§Ç.
             ‡§§‡•Å‡§Æ‡§ö‡§Ç ‡§â‡§§‡•ç‡§§‡§∞ ‡§•‡•ã‡§°‡§ï‡•ç‡§Ø‡§æ‡§§ ‡§†‡•á‡§µ‡§æ (‡§ú‡§æ‡§∏‡•ç‡§§‡•Ä‡§§ ‡§ú‡§æ‡§∏‡•ç‡§§ 2 ‡§µ‡§æ‡§ï‡•ç‡§Ø). ‡§ï‡•ã‡§£‡§§‡•Ä‡§π‡•Ä ‡§™‡•ç‡§∞‡§∏‡•ç‡§§‡§æ‡§µ‡§®‡§æ ‡§®‡§æ‡§π‡•Ä, ‡§≤‡§ó‡•á‡§ö ‡§â‡§§‡•ç‡§§‡§∞ ‡§∏‡•Å‡§∞‡•Ç ‡§ï‡§∞‡§æ. ‡§Æ‡§∞‡§æ‡§†‡•Ä‡§§ ‡§â‡§§‡•ç‡§§‡§∞ ‡§¶‡•ç‡§Ø‡§æ."""
}

def process_inputs(recorded_audio, uploaded_audio, text_input, image_filepath, language_choice, voice_service):
    """
    Process inputs from multiple sources: recorded audio, uploaded audio, or text input
    """
    try:
        user_query = ""
        
        # Priority order: text input > recorded audio > uploaded audio
        if text_input and text_input.strip():
            # Use text input directly
            user_query = text_input.strip()
            speech_to_text_output = f"Text input: {user_query}"
        elif recorded_audio:
            # Use recorded audio
            speech_to_text_output = transcribe_with_groq(
                GROQ_API_KEY=os.environ.get("GROQ_API_KEY"),
                audio_filepath=recorded_audio,
                stt_model="whisper-large-v3"
            )
            user_query = speech_to_text_output
        elif uploaded_audio:
            # Use uploaded audio file
            speech_to_text_output = transcribe_with_groq(
                GROQ_API_KEY=os.environ.get("GROQ_API_KEY"),
                audio_filepath=uploaded_audio,
                stt_model="whisper-large-v3"
            )
            user_query = speech_to_text_output
        else:
            # No input provided
            no_input_messages = {
                'english': "Please provide a question via text, recorded audio, or uploaded audio file.",
                'hindi': "‡§ï‡•É‡§™‡§Ø‡§æ ‡§ü‡•á‡§ï‡•ç‡§∏‡•ç‡§ü, ‡§∞‡§ø‡§ï‡•â‡§∞‡•ç‡§° ‡§ï‡•Ä ‡§ó‡§à ‡§ë‡§°‡§ø‡§Ø‡•ã, ‡§Ø‡§æ ‡§Ö‡§™‡§≤‡•ã‡§° ‡§ï‡•Ä ‡§ó‡§à ‡§ë‡§°‡§ø‡§Ø‡•ã ‡§´‡§º‡§æ‡§á‡§≤ ‡§ï‡•á ‡§¶‡•ç‡§µ‡§æ‡§∞‡§æ ‡§™‡•ç‡§∞‡§∂‡•ç‡§® ‡§¶‡•á‡§Ç‡•§",
                'marathi': "‡§ï‡•É‡§™‡§Ø‡§æ ‡§Æ‡§ú‡§ï‡•Ç‡§∞, ‡§∞‡•á‡§ï‡•â‡§∞‡•ç‡§° ‡§ï‡•á‡§≤‡•á‡§≤‡§æ ‡§ë‡§°‡§ø‡§ì ‡§ï‡§ø‡§Ç‡§µ‡§æ ‡§Ö‡§™‡§≤‡•ã‡§° ‡§ï‡•á‡§≤‡•á‡§≤‡•ç‡§Ø‡§æ ‡§ë‡§°‡§ø‡§ì ‡§´‡§æ‡§á‡§≤‡§¶‡•ç‡§µ‡§æ‡§∞‡•á ‡§™‡•ç‡§∞‡§∂‡•ç‡§® ‡§¶‡•ç‡§Ø‡§æ."
            }
            error_msg = no_input_messages.get(language_choice.lower(), no_input_messages['english'])
            return error_msg, error_msg, None
        
        # Get appropriate system prompt based on language
        system_prompt = system_prompts.get(language_choice.lower(), system_prompts['english'])
        
        # Handle the image input
        if image_filepath:
            doctor_response = analyze_image_with_query(
                query=system_prompt + " " + user_query, 
                encoded_image=encode_image(image_filepath), 
                model="meta-llama/llama-4-scout-17b-16e-instruct"
            )
        else:
            # Provide language-appropriate "no image" message
            no_image_messages = {
                'english': "No image provided for me to analyze. Please describe your symptoms or upload a medical image.",
                'hindi': "‡§µ‡§ø‡§∂‡•ç‡§≤‡•á‡§∑‡§£ ‡§ï‡•á ‡§≤‡§ø‡§è ‡§ï‡•ã‡§à ‡§õ‡§µ‡§ø ‡§™‡•ç‡§∞‡§¶‡§æ‡§® ‡§®‡§π‡•Ä‡§Ç ‡§ï‡•Ä ‡§ó‡§à‡•§ ‡§ï‡•É‡§™‡§Ø‡§æ ‡§Ö‡§™‡§®‡•á ‡§≤‡§ï‡•ç‡§∑‡§£‡•ã‡§Ç ‡§ï‡§æ ‡§µ‡§∞‡•ç‡§£‡§® ‡§ï‡§∞‡•á‡§Ç ‡§Ø‡§æ ‡§ö‡§ø‡§ï‡§ø‡§§‡•ç‡§∏‡§æ ‡§õ‡§µ‡§ø ‡§Ö‡§™‡§≤‡•ã‡§° ‡§ï‡§∞‡•á‡§Ç‡•§",
                'marathi': "‡§µ‡§ø‡§∂‡•ç‡§≤‡•á‡§∑‡§£‡§æ‡§∏‡§æ‡§†‡•Ä ‡§ï‡•ã‡§£‡§§‡•Ä‡§π‡•Ä ‡§™‡•ç‡§∞‡§§‡§ø‡§Æ‡§æ ‡§¶‡§ø‡§≤‡•Ä ‡§®‡§æ‡§π‡•Ä. ‡§ï‡•É‡§™‡§Ø‡§æ ‡§§‡•Å‡§Æ‡§ö‡•ç‡§Ø‡§æ ‡§≤‡§ï‡•ç‡§∑‡§£‡§æ‡§Ç‡§ö‡•á ‡§µ‡§∞‡•ç‡§£‡§® ‡§ï‡§∞‡§æ ‡§ï‡§ø‡§Ç‡§µ‡§æ ‡§µ‡•à‡§¶‡•ç‡§Ø‡§ï‡•Ä‡§Ø ‡§™‡•ç‡§∞‡§§‡§ø‡§Æ‡§æ ‡§Ö‡§™‡§≤‡•ã‡§° ‡§ï‡§∞‡§æ."
            }
            
            # If there's a user query but no image, try to respond to the text query
            if user_query:
                # Create a text-only medical consultation prompt
                text_only_prompts = {
                    'english': f"As a doctor, based on the symptoms or question: '{user_query}', provide a brief medical consultation response. Keep it concise and suggest seeing a healthcare professional for proper diagnosis.",
                    'hindi': f"‡§è‡§ï ‡§°‡•â‡§ï‡•ç‡§ü‡§∞ ‡§ï‡•á ‡§∞‡•Ç‡§™ ‡§Æ‡•á‡§Ç, ‡§≤‡§ï‡•ç‡§∑‡§£‡•ã‡§Ç ‡§Ø‡§æ ‡§™‡•ç‡§∞‡§∂‡•ç‡§® ‡§ï‡•á ‡§Ü‡§ß‡§æ‡§∞ ‡§™‡§∞: '{user_query}', ‡§è‡§ï ‡§∏‡§Ç‡§ï‡•ç‡§∑‡§ø‡§™‡•ç‡§§ ‡§ö‡§ø‡§ï‡§ø‡§§‡•ç‡§∏‡§æ ‡§™‡§∞‡§æ‡§Æ‡§∞‡•ç‡§∂ ‡§™‡•ç‡§∞‡§§‡§ø‡§ï‡•ç‡§∞‡§ø‡§Ø‡§æ ‡§™‡•ç‡§∞‡§¶‡§æ‡§® ‡§ï‡§∞‡•á‡§Ç‡•§ ‡§á‡§∏‡•á ‡§∏‡§Ç‡§ï‡•ç‡§∑‡§ø‡§™‡•ç‡§§ ‡§∞‡§ñ‡•á‡§Ç ‡§î‡§∞ ‡§â‡§ö‡§ø‡§§ ‡§®‡§ø‡§¶‡§æ‡§® ‡§ï‡•á ‡§≤‡§ø‡§è ‡§∏‡•ç‡§µ‡§æ‡§∏‡•ç‡§•‡•ç‡§Ø ‡§∏‡•á‡§µ‡§æ ‡§™‡•á‡§∂‡•á‡§µ‡§∞ ‡§∏‡•á ‡§Æ‡§ø‡§≤‡§®‡•á ‡§ï‡§æ ‡§∏‡•Å‡§ù‡§æ‡§µ ‡§¶‡•á‡§Ç‡•§",
                    'marathi': f"‡§°‡•â‡§ï‡•ç‡§ü‡§∞ ‡§Æ‡•ç‡§π‡§£‡•Ç‡§®, ‡§≤‡§ï‡•ç‡§∑‡§£‡•á ‡§ï‡§ø‡§Ç‡§µ‡§æ ‡§™‡•ç‡§∞‡§∂‡•ç‡§®‡§æ‡§ö‡•ç‡§Ø‡§æ ‡§Ü‡§ß‡§æ‡§∞‡•á: '{user_query}', ‡§•‡•ã‡§°‡§ï‡•ç‡§Ø‡§æ‡§§ ‡§µ‡•à‡§¶‡•ç‡§Ø‡§ï‡•Ä‡§Ø ‡§∏‡§≤‡•ç‡§≤‡§æ ‡§¶‡•ç‡§Ø‡§æ. ‡§Ø‡•ã‡§ó‡•ç‡§Ø ‡§®‡§ø‡§¶‡§æ‡§®‡§æ‡§∏‡§æ‡§†‡•Ä ‡§Ü‡§∞‡•ã‡§ó‡•ç‡§Ø ‡§∏‡•á‡§µ‡§æ ‡§µ‡•ç‡§Ø‡§æ‡§µ‡§∏‡§æ‡§Ø‡§ø‡§ï‡§æ‡§≤‡§æ ‡§≠‡•á‡§ü‡§£‡•ç‡§Ø‡§æ‡§ö‡§æ ‡§∏‡§≤‡•ç‡§≤‡§æ ‡§¶‡•ç‡§Ø‡§æ."
                }
                
                # For text-only queries, we can still provide some medical guidance
                doctor_response = text_only_prompts.get(language_choice.lower(), text_only_prompts['english'])
            else:
                doctor_response = no_image_messages.get(language_choice.lower(), no_image_messages['english'])
        
        # Generate audio response with language support
        output_audio_path = f"doctor_response_{language_choice.lower()}.mp3"
        
        # Choose TTS service
        use_elevenlabs = (voice_service == "ElevenLabs (Premium)")
        
        text_to_speech_multilingual(
            input_text=doctor_response, 
            output_filepath=output_audio_path,
            language=language_choice.lower(),
            use_elevenlabs=use_elevenlabs
        )
        
        # Return results
        if os.path.exists(output_audio_path):
            return (speech_to_text_output if 'speech_to_text_output' in locals() else f"Text input: {user_query}"), doctor_response, output_audio_path
        else:
            return (speech_to_text_output if 'speech_to_text_output' in locals() else f"Text input: {user_query}"), doctor_response, None
            
    except Exception as e:
        error_msg = f"Error processing: {str(e)}"
        return error_msg, error_msg, None

# Create the enhanced interface with multiple input options
with gr.Blocks(title="AI Doctor with Vision and Voice") as iface:
    gr.Markdown("# ü©∫ AI Doctor with Vision and Voice")
    gr.Markdown("Upload an image and ask a question via **text**, **voice recording**, or **audio file** to get medical analysis in English, Hindi, or Marathi")
    
    with gr.Row():
        with gr.Column(scale=1):
            # Input section
            gr.Markdown("### üìù Ask Your Question")
            
            # Text input
            text_input = gr.Textbox(
                label="Type your question here",
                placeholder="e.g., What do you see in this image? or Describe the symptoms...",
                lines=3
            )
            
            gr.Markdown("**OR**")
            
            # Audio inputs
            recorded_audio = gr.Audio(
                sources=["microphone"], 
                type="filepath", 
                label="Record your question"
            )
            
            gr.Markdown("**OR**")
            
            uploaded_audio = gr.Audio(
                sources=["upload"], 
                type="filepath", 
                label="Upload audio file"
            )
            
            # Image input
            image_input = gr.Image(
                type="filepath", 
                label="Upload medical image (optional)"
            )
            
            # Settings
            gr.Markdown("### ‚öôÔ∏è Settings")
            language_choice = gr.Dropdown(
                choices=["English", "Hindi", "Marathi"], 
                value="English", 
                label="Response Language"
            )
            
            voice_service = gr.Radio(
                choices=["gTTS (Free)", "ElevenLabs (Premium)"], 
                value="gTTS (Free)", 
                label="Voice Service"
            )
            
            # Submit button
            submit_btn = gr.Button("üîç Analyze", variant="primary", size="lg")
        
        with gr.Column(scale=1):
            # Output section
            gr.Markdown("### üìã Results")
            
            input_text_output = gr.Textbox(
                label="Your Input (Transcribed)",
                interactive=False
            )
            
            doctor_response_output = gr.Textbox(
                label="Doctor's Response",
                interactive=False,
                lines=5
            )
            
            audio_response_output = gr.Audio(
                label="Doctor's Voice Response",
                type="filepath"
            )
    
    # Examples section
    gr.Markdown("### üí° Example Questions")
    gr.Examples(
        examples=[
            ["What do you see in this X-ray image?", None, None],
            ["I have a persistent headache for 3 days", None, None],
            ["Can you analyze this skin condition?", None, None],
            ["‡§Æ‡•Å‡§ù‡•á ‡§¨‡•Å‡§ñ‡§æ‡§∞ ‡§î‡§∞ ‡§ñ‡§æ‡§Ç‡§∏‡•Ä ‡§π‡•à", None, None],
            ["‡§Æ‡§æ‡§ù‡•ç‡§Ø‡§æ ‡§°‡•ã‡§≥‡•ç‡§Ø‡§æ‡§§ ‡§¶‡•Å‡§ñ‡§§ ‡§Ü‡§π‡•á", None, None]
        ],
        inputs=[text_input, recorded_audio, uploaded_audio]
    )
    
    # Connect the function to the interface
    submit_btn.click(
        fn=process_inputs,
        inputs=[
            recorded_audio,
            uploaded_audio, 
            text_input,
            image_input,
            language_choice,
            voice_service
        ],
        outputs=[
            input_text_output,
            doctor_response_output,
            audio_response_output
        ]
    )

if __name__ == "__main__":
    iface.launch(debug=True)